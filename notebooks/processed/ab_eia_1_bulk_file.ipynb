{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIA International"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download du fichier international .zip sur eia (https://www.eia.gov/opendata/)\n",
    "\n",
    "# url of the international file on eia\n",
    "url = \"https://www.eia.gov/opendata/bulk/INTL.zip\"\n",
    "\n",
    "\n",
    "#/Users/alexandrebernard/Documents/Perso/Data/D4G/shiftdataportal_data\n",
    "#defining file name and destination_raw\n",
    "destination_raw = \"../../data/_raw/eia/\"\n",
    "file_name = \"eia_international_bulk.zip\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Download of the txt file containing all data of International API route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download completed\n"
     ]
    }
   ],
   "source": [
    "#download the file from url\n",
    "urllib.request.urlretrieve(url, destination_raw + file_name)\n",
    "print(\"download completed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unzip the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file unzipped\n"
     ]
    }
   ],
   "source": [
    "# unzip the file and delete zip\n",
    "\n",
    "#ouvrir le fichier en mode lecture\n",
    "with zipfile.ZipFile(destination_raw + file_name, 'r') as zip_ref:\n",
    "    #Extract file in same repertory\n",
    "    zip_ref.extractall(destination_raw)\n",
    "print(\"file unzipped\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of the JSONs from txt file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---open txt file---\n",
      "---iteration on 5000 row completed---\n",
      "---iteration on 10000 row completed---\n",
      "---iteration on 15000 row completed---\n",
      "---iteration on 20000 row completed---\n",
      "---iteration on 25000 row completed---\n",
      "---iteration on 30000 row completed---\n",
      "---iteration on 35000 row completed---\n",
      "---iteration on 40000 row completed---\n",
      "---iteration on 45000 row completed---\n",
      "---iteration on 50000 row completed---\n",
      "---iteration on 55000 row completed---\n",
      "---iteration on 60000 row completed---\n",
      "---iteration on 65000 row completed---\n",
      "---iteration on 70000 row completed---\n",
      "---iteration on 75000 row completed---\n",
      "---iteration on 80000 row completed---\n",
      "---iteration on 85000 row completed---\n",
      "---iteration on 90000 row completed---\n",
      "---iteration on 95000 row completed---\n",
      "---iteration on 100000 row completed---\n",
      "--- iteration done in 8.777052879333496 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "json_list = []\n",
    "\n",
    "print(\"---open txt file---\")\n",
    "#open the file in reading mode\n",
    "with open(destination_raw + \"INTL.txt\", 'r') as file_txt:\n",
    "\n",
    "    count = 0                        # TO DELETE\n",
    "    # iteration on each json in the txt file\n",
    "    for row in file_txt:\n",
    "        #delete blank and new line caracter\n",
    "        row = row.strip()\n",
    "        #load the txt of one row into a json. use of \"json.loads()\" as we are loading a string into a json file corresponding to the row\n",
    "        data_json = json.loads(row)\n",
    "        #insert in dataframe\n",
    "        #df_temp = pd.DataFrame(data_json)\n",
    "        #add to the list of jsons\n",
    "        json_list.append(data_json)\n",
    "        \n",
    "        #management of execution time\n",
    "        count += 1                   # TO DELETE\n",
    "        if count % 5000 == 0:\n",
    "            print(\"---iteration on %s row completed---\" %count)\n",
    "        #if stop == 20:              # TO DELETE\n",
    "        #    break                   # TO DELETE\n",
    "\n",
    "df = pd.DataFrame(json_list)\n",
    "\n",
    "iteration_time = time.time()\n",
    "print(\"--- iteration done in %s seconds ---\" %(iteration_time - start_time))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formating of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- dropna done in 0.10137701034545898 seconds ---\n",
      "--- annual done in 0.07101297378540039 seconds ---\n",
      "--- explode done in 7.2030041217803955 seconds ---\n",
      "--- tolist done in 2.005150079727173 seconds ---\n",
      "--- concatcol + delete regional data done in 24.624401807785034 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#delete non valid data in \"data\" column\n",
    "start_time = time.time()\n",
    "\n",
    "#deletion of NaN values\n",
    "df_structured = df.dropna(subset=[\"data\"])\n",
    "\n",
    "dropna_time = time.time()\n",
    "print(\"--- dropna done in %s seconds ---\" %(dropna_time - start_time))\n",
    "\n",
    "#we only keep annual data\n",
    "df_structured = df_structured[df_structured[\"f\"]==\"A\"]\n",
    "\n",
    "annual_time = time.time()\n",
    "print(\"--- annual done in %s seconds ---\" %(annual_time - dropna_time))\n",
    "\n",
    "#management of the columns \"data\" that is a list of lists\n",
    "#extend \"data\" column by creating a nes row for each internal list\n",
    "df_structured = df_structured.explode(\"data\")\n",
    "\n",
    "explode_time = time.time()\n",
    "print(\"--- explode done in %s seconds ---\" %(explode_time - annual_time))\n",
    "\n",
    "# transformation of the \"data\" columns (containing a list [year, value] into 2 columns for date and value\n",
    "df_structured[[\"date\", \"value\"]] = pd.DataFrame(df_structured[\"data\"].tolist(), index=df_structured.index)\n",
    "df_structured = df_structured.drop(\"data\", axis=1)\n",
    "\n",
    "tolist_time = time.time()\n",
    "print(\"--- tolist done in %s seconds ---\" %(tolist_time - explode_time))\n",
    "\n",
    "\n",
    "#split of concatenated columns\n",
    "# for the \"series_id\" column\n",
    "df_structured[[\"file_name\", \"product_id\", \"activity_id\", \"country_region_id\", \"unit_id\", \"frequency_id\"]] = df_structured[\"series_id\"].str.split('[.-]', expand=True)\n",
    "# for the \"name\" column\n",
    "#df_structured[[\"product_name\", \"country_region_name\", \"frequency_name\"]] = df_structured[\"name\"].str.split(',', expand=True)\n",
    "#=> not working because there can be 2 until 7 \",\"\n",
    "\n",
    "#we do not keep regional data. \n",
    "df_structured = df_structured[df_structured[\"country_region_id\"].str.len()!=4]\n",
    "#df_structured.shape (3 952 091, 23) puis (3 364 396) aprÃ¨s suppr des regions\n",
    "# toutes les regions ont 4 caracteres pour le code pays\n",
    "# les pays suivants ont 4 caracteres : DEUW allemagne ouest, DEUO GM offshore used for load only, \n",
    "# GBRO UK offshore used for load only, NLDA Netherlands antilles, NLDO NL offshore used for load only\n",
    "# USIQ US pacific islands, USOH us territories\n",
    "\n",
    "concatcol_time = time.time()\n",
    "print(\"--- concatcol + delete regional data done in %s seconds ---\" %(concatcol_time - tolist_time))\n",
    "\n",
    "#reset of indexes\n",
    "df_structured = df_structured.reset_index(drop=True)\n",
    "\n",
    "#df.to_csv(destination_raw + \"test.csv\")\n",
    "#df_structured.head()\n",
    "# run time before keeping only annual data >8min20\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns renaming and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_18297/1984231087.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_eia.rename(columns={\n"
     ]
    }
   ],
   "source": [
    "#selection and reordering of the columns\n",
    "df_eia = df_structured[[\"source\", \"file_name\", \"name\", \"country_region_id\", \"product_id\", \"activity_id\", \"date\", \"unit_id\", \"units\", \"value\", \"last_updated\"]]\n",
    "\n",
    "#colums renaming\n",
    "df_eia.rename(columns={\n",
    "    'units': 'unit_name',\n",
    "    'name': 'product_region_freq_name',\n",
    "    'date': 'year'\n",
    "}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store cleaned eia data in csv file + delete zip and unzipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index on false to not send it into the csv file\n",
    "df_eia.to_csv(destination_raw + \"eia_cleaned.csv\", index=False)\n",
    "\n",
    "os.remove(destination_raw + \"eia_international_bulk.zip\")\n",
    "os.remove(destination_raw + \"INTL.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
